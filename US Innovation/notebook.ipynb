{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a single, self-contained Jupyter notebook that implements the\n",
    "# \"single polished notebook\" approach for the thesis causal analysis.\n",
    "# The notebook will:\n",
    "# - Try to load data from data/cleaned.csv or data/built5.dta\n",
    "# - If not present, synthesize a small realistic panel dataset so the notebook runs end-to-end\n",
    "# - Compute period means, surge, define treatment, run DiD OLS with county and year FE and cluster SE\n",
    "# - Produce an event-study style mean plot\n",
    "# - Run IV diagnostics: residualize on year FE and scatter residuals\n",
    "# - Run IV (2SLS) using linearmodels' IV2SLS\n",
    "# - Create quartile groups and run heterogeneity IV with interacted treatment/instrument\n",
    "# - All code cells include explanatory comments and printouts\n",
    "#\n",
    "# The notebook will be saved to /mnt/data/thesis_single_notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat as nbf\n",
    "from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell\n",
    "import os, json, textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = new_notebook()\n",
    "nb.cells = []\n",
    "\n",
    "# Title and overview\n",
    "nb.cells.append(new_markdown_cell(\n",
    "\"# Reproducible Mini-Analysis: Causal Effect of Procurement on Regional Innovation\\n\\n\"\n",
    "\"This notebook provides a polished, self-contained demonstration of the key analysis from the thesis. \"\n",
    "\"It is written to run end-to-end: if your cleaned data file is present under `data/cleaned.csv` (or `data/built5.dta`), \"\n",
    "\"the notebook will use it. Otherwise the notebook will simulate a small synthetic panel with realistic variable names \"\n",
    "\"so the analyses run and the figures/tables render. \\n\\n**Sections:**\\n\"\n",
    "\"- Setup & data loading\\n- Data construction / cleaning\\n- Difference-in-Differences (DiD) exploratory analysis\\n- Instrumental Variables (IV) diagnostics and main specs\\n- Heterogeneity analysis (grouped IV)\\n\\n> Note: this notebook is optimized for clarity and correctness (suitable for a portfolio demo). Replace the synthetic data with your cleaned dataset to reproduce real results.\"\n",
    "))\n",
    "\n",
    "# Setup cell: imports and helper functions\n",
    "nb.cells.append(new_code_cell(\n",
    "\"\"\"# Setup: imports and helpers\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from linearmodels.iv import IV2SLS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "\n",
    "# helper functions\n",
    "def ensure_dir(path):\n",
    "    d = os.path.dirname(path)\n",
    "    if d and not os.path.exists(d):\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def read_data(path_csv='data/cleaned.csv', path_dta='data/built5.dta'):\n",
    "    if os.path.exists(path_csv):\n",
    "        print(f'Loading CSV data from {path_csv}')\n",
    "        return pd.read_csv(path_csv)\n",
    "    if os.path.exists(path_dta):\n",
    "        print(f'Loading Stata data from {path_dta}')\n",
    "        return pd.read_stata(path_dta)\n",
    "    return None\n",
    "\n",
    "def save_fig(fig, path):\n",
    "    ensure_dir(path)\n",
    "    fig.savefig(path, dpi=150, bbox_inches='tight')\n",
    "    print('Saved figure to', path)\n",
    "\"\"\"\n",
    "))\n",
    "\n",
    "# Data loading / synth cell\n",
    "nb.cells.append(new_code_cell(\n",
    "\"\"\"# Data load or synthetic data creation\n",
    "df = read_data()\n",
    "if df is None:\n",
    "    print('No data file found. Creating a synthetic example dataset for demo purposes.')\n",
    "    # Create synthetic panel: 50 counties, years 1965-2003\n",
    "    rng = np.random.default_rng(12345)\n",
    "    counties = [f'C{i:03d}' for i in range(1,51)]\n",
    "    years = np.arange(1965, 2004)\n",
    "    rows = []\n",
    "    for c in counties:\n",
    "        base_patents = rng.poisson(5)  # baseline patents\n",
    "        comp_wt = rng.random()\n",
    "        for y in years:\n",
    "            # total dollars grows over time with noise\n",
    "            total_dollars = max(0.0, 100 + 2*(y-1965) + rng.normal(0,50))\n",
    "            # create ltotal_dollars (log)\n",
    "            ltotal = np.log(total_dollars + 1)\n",
    "            # instrument variation\n",
    "            lspend_iv = max(0.1, 50 + (y-1975)*rng.normal(0.5,0.5))\n",
    "            log_sp_iv5 = np.log(lspend_iv + 1)\n",
    "            # patent counts / citations (noisy)\n",
    "            num_patents = np.random.poisson( base_patents * (1 + 0.01*(y-1975)) )\n",
    "            # weighted citations (continuous)\n",
    "            w_cites_sub = base_patents * 0.5 + rng.normal(0,1)\n",
    "            lw_cites_sub = np.log(max(0.1, w_cites_sub) + 1)\n",
    "            avg_wages = 20000 + rng.normal(0,2000)\n",
    "            pop = max(1000, int(50000 + rng.normal(0,10000)))\n",
    "            emp = max(100, int(pop*0.4))\n",
    "            inventor_share = rng.random()\n",
    "            tech_emp_share = rng.random()\n",
    "            emp_share = rng.random()\n",
    "            rows.append({\n",
    "                'county_fips': c,\n",
    "                'fyear': int(y),\n",
    "                'total_dollars': float(total_dollars),\n",
    "                'ltotal_dollars': float(ltotal),\n",
    "                'lspending_6675_iv': float(lspend_iv),\n",
    "                'log_spending_iv5': float(log_sp_iv5),\n",
    "                'num_patents': int(num_patents),\n",
    "                'w_cites_sub': float(w_cites_sub),\n",
    "                'lw_cites_sub': float(lw_cites_sub),\n",
    "                'avg_wages': float(avg_wages),\n",
    "                'pop': int(pop),\n",
    "                'emp': int(emp),\n",
    "                'comp_wtd': float(comp_wt),\n",
    "                'inventor_share': float(inventor_share),\n",
    "                'tech_emp_share': float(tech_emp_share),\n",
    "                'emp_share': float(emp_share),\n",
    "                # semi_intens indicator for sample selection\n",
    "                'semi_intens': int(rng.choice([0,1], p=[0.2, 0.8]))\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    print('Synthetic dataset created with rows:', len(df))\n",
    "else:\n",
    "    print('Data loaded with rows:', len(df))\n",
    "\"\"\"))\n",
    "\n",
    "# Compute period means and surge\n",
    "nb.cells.append(new_code_cell(\n",
    "\"\"\"# Compute period means (1976-1981 and 1981-1989) and surge per county\n",
    "df = df.copy()\n",
    "# compute mean_spend1 and mean_spend2 per county\n",
    "def mean_cond(series, years, fyear):\n",
    "    mask = (fyear >= years[0]) & (fyear <= years[1])\n",
    "    return series.where(mask).mean()\n",
    "\n",
    "# Use groupby apply to compute per-county means and broadcast\n",
    "df['mean_spend1'] = df.groupby('county_fips').apply(lambda g: g.loc[g['fyear'].between(1976,1981), 'total_dollars'].mean()).reindex(df['county_fips']).values\n",
    "df['mean_spend2'] = df.groupby('county_fips').apply(lambda g: g.loc[g['fyear'].between(1981,1989), 'total_dollars'].mean()).reindex(df['county_fips']).values\n",
    "\n",
    "# surge = mean_spend2 - mean_spend1\n",
    "df['surge'] = df['mean_spend2'] - df['mean_spend1']\n",
    "\n",
    "# Summary stats\n",
    "print('Surge: count non-null =', df['surge'].dropna().shape[0])\n",
    "print(df[['county_fips','mean_spend1','mean_spend2','surge']].drop_duplicates().head())\n",
    "\n",
    "# Define treated by median surge across counties (as in your Stata)\n",
    "county_surge = df.drop_duplicates(subset=['county_fips'])[['county_fips','surge']].dropna().set_index('county_fips')['surge']\n",
    "surge_median = county_surge.median()\n",
    "surge_mean = county_surge.mean()\n",
    "surge_sd = county_surge.std()\n",
    "print(f\"surge_median={surge_median:.3f}, surge_mean={surge_mean:.3f}, surge_sd={surge_sd:.3f}\")\n",
    "df['treated'] = df['county_fips'].map((county_surge > surge_median).astype(int)).fillna(0).astype(int)\n",
    "# Create after indicator (post-1981)\n",
    "df['after'] = (df['fyear'] > 1981).astype(int)\n",
    "df['treatment'] = df['treated'] * df['after']\n",
    "df.head().iloc[:, :12]\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run DiD regression\n",
    "nb.cells.append(new_code_cell(\n",
    "\"\"\"# Difference-in-Differences OLS with county and year fixed effects and cluster robust SE\n",
    "# Outcome: w_cites_sub (citation-weighted patent count), treatment: treatment\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# select sample as in Stata (keep if semi_intens == 1)\n",
    "df_did = df[df['semi_intens'] == 1].copy()\n",
    "print('DiD sample rows:', len(df_did))\n",
    "\n",
    "formula = 'w_cites_sub ~ treatment + C(county_fips) + C(fyear)'\n",
    "model = smf.ols(formula=formula, data=df_did)\n",
    "# cluster by county_fips\n",
    "res = model.fit(cov_type='cluster', cov_kwds={'groups': df_did['county_fips']})\n",
    "print(res.summary().tables[1])\n",
    "\"\"\"))\n",
    "\n",
    "# Event study plot (mean by year for treated vs control)\n",
    "nb.cells.append(new_code_cell(\n",
    "\"\"\"# Event-study style mean trends (treated vs control)\n",
    "agg = df_did.groupby(['fyear','treated'])['w_cites_sub'].mean().reset_index()\n",
    "plt.figure(figsize=(8,4))\n",
    "for t, grp in agg.groupby('treated'):\n",
    "    plt.plot(grp['fyear'], grp['w_cites_sub'], marker='o', label=f'treated={int(t)}')\n",
    "plt.axvline(x=1981, color='k', linestyle='--', label='policy year 1981')\n",
    "plt.xlabel('Year'); plt.ylabel('Mean citation-weighted patents'); plt.title('Event-study mean trends (treated vs control)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IV diagnostics: residualize on year FE and scatter residuals\n",
    "nb.cells.append(new_code_cell(\n",
    "\"\"\"# IV First-stage diagnostics: residualize ltotal_dollars and instrument on year FE for subset\n",
    "mask = (df['semi_intens'] == 1) & (df['fyear'] > 1975) & (df['ltotal_dollars'] > 0) & (df['lspending_6675_iv'] > 0)\n",
    "print('IV diag rows:', mask.sum())\n",
    "if mask.sum() > 0:\n",
    "    sub = df.loc[mask].copy()\n",
    "    # residualize on year FE: regress variable on C(fyear) and take residuals\n",
    "    res1 = smf.ols('ltotal_dollars ~ C(fyear)', data=sub).fit()\n",
    "    sub['resid_lt'] = res1.resid\n",
    "    res2 = smf.ols('lspending_6675_iv ~ C(fyear)', data=sub).fit()\n",
    "    sub['resid_sp'] = res2.resid\n",
    "    # scatter residuals and fitted line\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.scatter(sub['resid_sp'], sub['resid_lt'], s=10, alpha=0.6)\n",
    "    # fit line\n",
    "    coef = np.polyfit(sub['resid_sp'].fillna(0), sub['resid_lt'].fillna(0), 1)\n",
    "    xs = np.linspace(sub['resid_sp'].min(), sub['resid_sp'].max(), 50)\n",
    "    plt.plot(xs, coef[0]*xs + coef[1], color='red')\n",
    "    plt.xlabel('Residuals of instrument (lspending_6675_iv)'); plt.ylabel('Residuals of ltotal_dollars')\n",
    "    plt.title('Partial relationship: residuals (first-stage diagnostic)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No rows satisfy IV diagnostic mask; check data.')\n",
    "\"\"\"))\n",
    "\n",
    "# Run IV 2SLS main spec\n",
    "nb.cells.append(new_code_cell(\n",
    "\"\"\"# IV 2SLS: example main specification\n",
    "# We'll run a basic IV where lw_cites_sub is outcome and ltotal_dollars is endogenous,\n",
    "# instrumented by lspending_6675_iv, including year and county FE and clustering by county.\n",
    "\n",
    "# Select sample analogous to Stata: semi_intens == 1 & fyear > 1975\n",
    "df_iv = df[(df['semi_intens'] == 1) & (df['fyear'] > 1975)].copy()\n",
    "print('IV sample rows:', len(df_iv))\n",
    "\n",
    "# Build formula for IV2SLS (linearmodels): outcome ~ exog + [endog ~ instr]\n",
    "# We'll include avg_wages, pop, emp as exogenous controls in the full spec.\n",
    "formula_iv = 'lw_cites_sub ~ 1 + avg_wages + pop + emp + C(fyear) + C(county_fips) + [ltotal_dollars ~ lspending_6675_iv]'\n",
    "\n",
    "try:\n",
    "    iv_mod = IV2SLS.from_formula(formula_iv, data=df_iv)\n",
    "    iv_res = iv_mod.fit(cov_type='clustered', clusters=df_iv['county_fips'])\n",
    "    print(iv_res.summary)\n",
    "except Exception as e:\n",
    "    print('IV estimation failed (likely due to small synthetic sample or collinearity). Error:', e)\n",
    "    iv_res = None\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Heterogeneity: create quartile groups and run grouped IV\n",
    "nb.cells.append(new_code_cell(\n",
    "\"\"\"# Heterogeneity analysis by quartiles of comp_wtd (creates xg/zg interactions and runs IV)\n",
    "df_het = df.copy()\n",
    "if 'comp_wtd' not in df_het.columns:\n",
    "    print('comp_wtd not found; creating a synthetic comp_wtd.')\n",
    "    df_het['comp_wtd'] = np.random.rand(len(df_het))\n",
    "\n",
    "# keep semi_intens sample (as original Stata)\n",
    "df_het = df_het[df_het['semi_intens'] == 1].copy()\n",
    "\n",
    "# Create quartile group (1..4)\n",
    "df_het['group'] = pd.qcut(df_het['comp_wtd'], q=4, labels=False) + 1\n",
    "\n",
    "# Create interactions xg1..xg4 and zg1..zg4\n",
    "for i in range(1,5):\n",
    "    df_het[f'g{i}'] = (df_het['group'] == i).astype(int)\n",
    "    df_het[f'xg{i}'] = df_het['ltotal_dollars'] * df_het[f'g{i}']\n",
    "    df_het[f'zg{i}'] = df_het['log_spending_iv5'] * df_het[f'g{i}']\n",
    "\n",
    "# Build IV formula: lw_cites_sub ~ exog + FE + [xg1 + ... + xg4 ~ zg1 + ... + zg4]\n",
    "exog = ['avg_wages','pop','emp']\n",
    "exog_str = ' + '.join(exog)\n",
    "endog_terms = ' + '.join([f'xg{i}' for i in range(1,5)])\n",
    "instr_terms = ' + '.join([f'zg{i}' for i in range(1,5)])\n",
    "formula_het = f\"lw_cites_sub ~ 1 + {exog_str} + C(fyear) + C(county_fips) + [{endog_terms} ~ {instr_terms}]\"\n",
    "\n",
    "print('Heterogeneity IV formula:', formula_het)\n",
    "try:\n",
    "    het_mod = IV2SLS.from_formula(formula_het, data=df_het)\n",
    "    het_res = het_mod.fit(cov_type='clustered', clusters=df_het['county_fips'])\n",
    "    print(het_res.summary)\n",
    "    # Extract coefficients for xg1..xg4\n",
    "    coefs = []\n",
    "    ses = []\n",
    "    labels = []\n",
    "    for i in range(1,5):\n",
    "        name = f'xg{i}'\n",
    "        if name in het_res.params.index:\n",
    "            labels.append(f\"q{i}\")\n",
    "            coefs.append(float(het_res.params[name]))\n",
    "            ses.append(float(het_res.std_errors[name]))\n",
    "    # Plot coefficients with 95% CI\n",
    "    if len(coefs) > 0:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        x = np.arange(len(coefs))\n",
    "        plt.errorbar(x, coefs, yerr=[ [1.96*s for s in ses], [1.96*s for s in ses] ], fmt='o', capsize=5)\n",
    "        plt.xticks(x, labels)\n",
    "        plt.axhline(0, color='k', linestyle='--')\n",
    "        plt.xlabel('Group'); plt.ylabel('Estimated Treatment Effect'); plt.title('Heterogeneity IV estimates (by quartile)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "except Exception as e:\n",
    "    print('Heterogeneity IV failed:', e)\n",
    "\"\"\"))\n",
    "\n",
    "# Interpretation cell\n",
    "nb.cells.append(new_markdown_cell(\n",
    "\"## Interpretation & Next Steps\\n\\n\"\n",
    "\"- The notebook runs a clean DiD and IV analysis on a panel dataset. Replace the synthetic data with your real `data/cleaned.csv` to reproduce actual thesis results.\\n\"\n",
    "\"- If IV estimation fails on the synthetic sample due to small/collinear data, it should run on your real dataset. Check variable names and adjust formula strings accordingly.\\n\"\n",
    "\"- To increase reproducibility later, move the functions into `src/` modules and add `requirements.txt` + a `scripts/` entrypoint.\\n\\n\"\n",
    "\"---\\n\"\n",
    "\"**If you want, I can now:**\\n\"\n",
    "\"1. Convert this notebook into a Colab-ready version (add a badge and small instructions), or\\n\"\n",
    "\"2. Extract the main tables/figures into `results/` files and add a short README.\\n\\n\"\n",
    "\"Which would you like next?\"\n",
    "))\n",
    "\n",
    "# Save notebook\n",
    "out_path = \"/mnt/data/thesis_single_notebook.ipynb\"\n",
    "with open(out_path, 'w') as f:\n",
    "    nbf.write(nb, f)\n",
    "\n",
    "out_path"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
